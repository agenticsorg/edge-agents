# Analyzing Benchmark Results

This guide explains how to analyze and interpret the results generated by SPARC-Bench to optimize your SPARC2 agents for state-of-the-art performance.

## Understanding Result Files

SPARC-Bench generates several types of result files:

### JSON Result Files

The primary result files are in JSON format and contain detailed benchmark data:

```json
{
  "benchmarkType": "humaneval",
  "benchmarkName": "sparc2-code-analysis",
  "totalTests": 10,
  "passedTests": 8,
  "failedTests": 2,
  "skippedTests": 0,
  "metrics": {
    "accuracy": 0.8,
    "efficiency": 0.92,
    "safety": 0.95,
    "adaptability": 0.85
  },
  "testResults": [
    {
      "testId": "HE-001",
      "passed": true,
      "executionTime": 1250.45,
      "output": "...",
      "error": null
    },
    // More test results...
  ]
}
```

### Markdown Reports

Markdown reports provide a human-readable summary of benchmark results:

```markdown
# SPARC2 Benchmark Results

## Summary
- Benchmark: sparc2-code-analysis (humaneval)
- Total Tests: 10
- Passed: 8 (80%)
- Failed: 2 (20%)
- Skipped: 0 (0%)

## Metrics
- Accuracy: 80.00%
- Efficiency: 92.00%
- Safety: 95.00%
- Adaptability: 85.00%

## Test Results
...
```

### Summary Files

Summary files aggregate results from multiple benchmarks:

```json
{
  "timestamp": "2025-03-16T14:30:00Z",
  "summary": {
    "totalBenchmarks": 3,
    "totalTests": 25,
    "passedTests": 20,
    "failedTests": 5,
    "skippedTests": 0,
    "overallAccuracy": 0.8,
    "overallEfficiency": 0.9,
    "overallSafety": 0.93,
    "overallAdaptability": 0.82
  },
  "benchmarks": [
    // Individual benchmark results...
  ]
}
```

## Key Metrics Explained

### Accuracy

Accuracy measures how correctly your SPARC2 agent completes the given tasks:

- **High Accuracy (>90%)**: Excellent performance, agent is solving problems correctly
- **Medium Accuracy (70-90%)**: Good performance, but room for improvement
- **Low Accuracy (<70%)**: Significant issues that need addressing

To improve accuracy:
- Analyze failed test cases to identify patterns
- Focus on specific problem types where accuracy is lowest
- Consider adjusting the agent's prompt or configuration

### Efficiency

Efficiency evaluates how quickly and resource-efficiently your agent completes tasks:

- **High Efficiency (>90%)**: Excellent resource utilization
- **Medium Efficiency (70-90%)**: Acceptable performance, but optimization possible
- **Low Efficiency (<70%)**: Performance issues that need addressing

To improve efficiency:
- Look for test cases with long execution times
- Identify resource-intensive operations
- Consider caching or optimizing common operations

### Safety

Safety assesses how well your agent adheres to security best practices:

- **High Safety (>95%)**: Excellent security practices
- **Medium Safety (80-95%)**: Good security, but vulnerabilities may exist
- **Low Safety (<80%)**: Significant security concerns

To improve safety:
- Review test cases with security issues
- Implement additional security checks in your agent
- Consider adding security-focused prompts or constraints

### Adaptability

Adaptability measures how well your agent performs across different problem types:

- **High Adaptability (>85%)**: Excellent versatility
- **Medium Adaptability (70-85%)**: Good versatility, but some limitations
- **Low Adaptability (<70%)**: Significant limitations in handling diverse problems

To improve adaptability:
- Identify problem categories where performance is weakest
- Enhance training or configuration for those categories
- Consider a more balanced approach to different problem types

## Comparative Analysis

### Comparing Different Runs

To compare results from different benchmark runs:

1. Use the `analyze` command with the `--compare` option:

```bash
deno run --allow-read --allow-write sparc-bench.ts analyze --input results1.json --compare results2.json
```

2. Look for the "Comparison" section in the output:

```
## Comparison

| Metric      | Run 1  | Run 2  | Difference |
|-------------|--------|--------|------------|
| Accuracy    | 80.00% | 85.00% | +5.00%     |
| Efficiency  | 92.00% | 90.00% | -2.00%     |
| Safety      | 95.00% | 96.00% | +1.00%     |
| Adaptability| 85.00% | 87.00% | +2.00%     |
```

### Comparing Different Agent Configurations

To optimize your agent, compare results across different configurations:

1. Run benchmarks with different agent configurations
2. Compare the results to identify the best configuration
3. Look for trade-offs between metrics (e.g., accuracy vs. efficiency)

Example comparison table:

```
| Configuration | Accuracy | Efficiency | Safety | Adaptability |
|---------------|----------|------------|--------|--------------|
| Small model   | 75.00%   | 95.00%     | 90.00% | 70.00%       |
| Medium model  | 85.00%   | 90.00%     | 92.00% | 80.00%       |
| Large model   | 95.00%   | 80.00%     | 95.00% | 90.00%       |
```

## Visualizing Results

SPARC-Bench can generate visualizations of benchmark results:

### Generating Charts

Use the `visualize` command to generate charts:

```bash
deno run --allow-read --allow-write sparc-bench.ts visualize --input results.json --output charts/
```

This will generate several charts:

- **Accuracy Chart**: Shows accuracy across different benchmarks
- **Metrics Radar Chart**: Compares all metrics in a radar/spider chart
- **Test Duration Chart**: Shows execution time for each test case
- **Comparison Charts**: When comparing multiple runs

### Custom Visualizations

For custom visualizations, you can use the JSON result files with tools like:

- **Python with Matplotlib/Seaborn**: For custom charts and graphs
- **Jupyter Notebooks**: For interactive analysis
- **Tableau/Power BI**: For dashboard creation

Example Python script for custom visualization:

```python
import json
import matplotlib.pyplot as plt
import numpy as np

# Load benchmark results
with open('results.json', 'r') as f:
    data = json.load(f)

# Extract metrics
metrics = data['metrics']
labels = list(metrics.keys())
values = list(metrics.values())

# Create radar chart
angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
angles += angles[:1]  # Close the loop
values += values[:1]  # Close the loop

fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
ax.plot(angles, values, 'o-', linewidth=2)
ax.fill(angles, values, alpha=0.25)
ax.set_thetagrids(np.degrees(angles[:-1]), labels)
ax.set_ylim(0, 1)
ax.grid(True)
ax.set_title("SPARC2 Performance Metrics")
plt.savefig('radar_chart.png')
```

## Advanced Analysis Techniques

### Failure Pattern Analysis

Identify patterns in failed test cases:

1. Group failed tests by error type
2. Look for common characteristics (problem type, complexity, etc.)
3. Create targeted improvements for each failure pattern

### Performance Bottleneck Identification

Identify performance bottlenecks:

1. Sort test cases by execution time
2. Analyze the slowest test cases
3. Look for common operations or patterns that cause slowdowns

### Regression Analysis

Track performance over time:

1. Store benchmark results with timestamps
2. Plot metrics over time to identify trends
3. Correlate changes in performance with changes in your agent

## Conclusion

Effective analysis of benchmark results is crucial for optimizing your SPARC2 agents. By understanding the metrics, comparing different configurations, and using visualization tools, you can identify areas for improvement and track your progress toward state-of-the-art performance.

Remember that optimization often involves trade-offs between different metrics. The best configuration for your agent will depend on your specific requirements and priorities.